{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b440775f",
   "metadata": {},
   "source": [
    "# Mem0 Cloud Memory Toolkit with CAMEL AI\n",
    "\n",
    "**Persistent Memory Management for AI Agents using Mem0 Cloud Storage**\n",
    "\n",
    "You can also check this cookbook in Colab. [CAMEL Homepage](https://www.camel-ai.org/) | [Join Discord](https://discord.gg/CNcNpquyDc) | ‚≠ê [Star us on GitHub](https://github.com/camel-ai/camel)\n",
    "\n",
    "This notebook demonstrates how to build an AI agent with persistent memory capabilities using CAMEL AI and Mem0's cloud storage. In this notebook, you'll explore:\n",
    "\n",
    "- **CAMEL AI**: A powerful multi-agent framework enabling sophisticated AI-driven tasks with memory management\n",
    "- **Mem0 Cloud**: Cloud-based persistent memory storage with semantic search capabilities  \n",
    "- **Memory Toolkit**: A custom toolkit for adding, retrieving, searching, and managing memories\n",
    "- **Interactive Agent**: A conversational agent that remembers past interactions across sessions\n",
    "\n",
    "This setup provides a practical foundation for building AI agents that can maintain context and learn from previous conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698b5e6",
   "metadata": {},
   "source": [
    "## üì¶ Installation\n",
    "\n",
    "First, install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c42f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install camel-ai==0.2.73 mem0ai rich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b60e8a",
   "metadata": {},
   "source": [
    "## üì• Download the Toolkit\n",
    "\n",
    "You'll need the custom Mem0 toolkit. Download or create the `mem0_tools.py` file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79896969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mem0 toolkit found!\n"
     ]
    }
   ],
   "source": [
    "# Download the toolkit file (replace with your actual download method)\n",
    "# For this demo, we assume you have the mem0_tools.py file in your working directory\n",
    "\n",
    "# Verify the file exists\n",
    "import os\n",
    "if os.path.exists('mem0_tools.py'):\n",
    "    print(\"‚úÖ Mem0 toolkit found!\")\n",
    "else:\n",
    "    print(\"‚ùå Please ensure mem0_tools.py is in your working directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572976e1",
   "metadata": {},
   "source": [
    "The toolkit provides these key capabilities:\n",
    "- **Add Memory**: Store information with optional metadata\n",
    "- **Retrieve Memories**: Get all stored memories \n",
    "- **Search Memories**: Semantic search with vector matching\n",
    "- **Delete Memories**: Clear stored memories\n",
    "\n",
    "## üîë Setting Up API Keys\n",
    "\n",
    "You'll need API keys for both Mem0 and your chosen LLM provider (Gemini in this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9971743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Set up Mem0 API key\n",
    "mem0_api_key = getpass('Enter your Mem0 API key: ')\n",
    "os.environ[\"MEM0_API_KEY\"] = mem0_api_key\n",
    "\n",
    "# Set up Gemini API key\n",
    "openai_api_key = getpass('Enter your OpenAI API key: ')\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f16c0",
   "metadata": {},
   "source": [
    "You can obtain:\n",
    "- **Mem0 API Key**: Sign up at [mem0.ai](https://mem0.ai)\n",
    "- **Gemini API Key**: Get it from [Google AI Studio](https://aistudio.google.com/app/apikey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070976dc",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Import the Mem0 Cloud Toolkit\n",
    "\n",
    "Now let's import our custom toolkit that interfaces with Mem0's cloud storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff56acb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mem0CloudToolkit imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import the custom Mem0 toolkit\n",
    "from mem0_tools import Mem0CloudToolkit\n",
    "\n",
    "# The toolkit provides 4 main methods:\n",
    "# - add_memory(content, metadata=None): Store new information\n",
    "# - retrieve_memories(): Get all stored memories  \n",
    "# - search_memories(query): Semantic search for relevant memories\n",
    "# - delete_memories(): Clear all stored memories\n",
    "\n",
    "print(\"‚úÖ Mem0CloudToolkit imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c7ce83",
   "metadata": {},
   "source": [
    "## ü§ñ Building the Memory-Enabled Agent\n",
    "\n",
    "Now let's create an interactive agent that uses our Mem0 toolkit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "299583d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Using agent_id: demo-agent, user_id: demo-user</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mUsing agent_id: demo-agent, user_id: demo-user\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">üß† Mem0 Cloud Agent is ready!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32müß† Mem0 Cloud Agent is ready!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from rich import print as rprint\n",
    "from camel.agents import ChatAgent\n",
    "from camel.messages import BaseMessage\n",
    "from camel.models import ModelFactory\n",
    "from camel.types import ModelPlatformType, ModelType\n",
    "\n",
    "# Set up agent and user IDs\n",
    "agent_id = \"demo-agent\"\n",
    "user_id = \"demo-user\"\n",
    "rprint(f\"[dim]Using agent_id: {agent_id}, user_id: {user_id}[/dim]\")\n",
    "\n",
    "# Initialize the Mem0 toolkit and get tools\n",
    "toolkit = Mem0CloudToolkit(agent_id=agent_id, user_id=user_id)\n",
    "tools = toolkit.get_tools()\n",
    "\n",
    "# Create the language model\n",
    "model = ModelFactory.create(\n",
    "    model_platform=ModelPlatformType.OPENAI,\n",
    "    model_type=ModelType.GPT_4_1,\n",
    ")\n",
    "\n",
    "# Configure the agent system message\n",
    "system_message = (\n",
    "    \"You are a helpful memory assistant that manages memories using Mem0 cloud storage. \"\n",
    "    \"When you use a tool, always explain what you did conversationally. \"\n",
    "    \"For example: 'I've stored that information!' or 'Here's what I found in your memories:' \"\n",
    "    \"Always be friendly and explain the memory operation you performed.\"\n",
    ")\n",
    "\n",
    "sys_msg = BaseMessage.make_assistant_message(\n",
    "    role_name=\"Memory Master\",\n",
    "    content=system_message,\n",
    ")\n",
    "\n",
    "# Create the agent\n",
    "agent = ChatAgent(sys_msg, model=model, tools=tools)\n",
    "\n",
    "rprint(\"[bold green]üß† Mem0 Cloud Agent is ready![/bold green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7143a77",
   "metadata": {},
   "source": [
    "## üéØ Interactive Mode\n",
    "\n",
    "For a full interactive experience with your memory-enabled agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a839f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">üß† Mem0 Cloud Agent is ready!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32müß† Mem0 Cloud Agent is ready!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Type </span><span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">'exit'</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> or </span><span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">'quit'</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> to end the conversation.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mType \u001b[0m\u001b[2;32m'exit'\u001b[0m\u001b[2m or \u001b[0m\u001b[2;32m'quit'\u001b[0m\u001b[2m to end the conversation.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-30 21:40:56,450 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.openai_model.OpenAIModel object at 0x7c779f61eea0>\n",
      "2025-07-30 21:40:56,451 - camel.camel.agents.chat_agent - ERROR - An error occurred while running model gpt-4.1-2025-04-14, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/camel/agents/chat_agent.py\", line 1798, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/camel/models/model_manager.py\", line 239, in run\n",
      "    raise exc\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/camel/models/model_manager.py\", line 229, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/camel/models/base_model.py\", line 55, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/camel/models/base_model.py\", line 407, in run\n",
      "    result = self._run(messages, response_format, tools)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/camel/models/openai_model.py\", line 318, in _run\n",
      "    result = self._request_chat_completion(messages, tools)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/camel/models/openai_model.py\", line 412, in _request_chat_completion\n",
      "    return self._client.chat.completions.create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 1131, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_base_client.py\", line 1256, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"Invalid schema for function 'add_memory': In context=('properties', 'metadata', 'type', '0'), 'additionalProperties' is required to be supplied and to be false.\", 'type': 'invalid_request_error', 'param': 'tools[0].function.parameters', 'code': 'invalid_function_parameters'}}\n"
     ]
    },
    {
     "ename": "ModelProcessingError",
     "evalue": "Unable to process messages: the only provided model did not run successfully. Error: Error code: 400 - {'error': {'message': \"Invalid schema for function 'add_memory': In context=('properties', 'metadata', 'type', '0'), 'additionalProperties' is required to be supplied and to be false.\", 'type': 'invalid_request_error', 'param': 'tools[0].function.parameters', 'code': 'invalid_function_parameters'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModelProcessingError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     12\u001b[39m user_msg = BaseMessage.make_user_message(role_name=\u001b[33m\"\u001b[39m\u001b[33mUser\u001b[39m\u001b[33m\"\u001b[39m, content=user_query)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m response = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Show the agent's response\u001b[39;00m\n\u001b[32m     16\u001b[39m rprint(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[green]ü§ñ Agent:[/green] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.msg.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/camel/agents/chat_agent.py:1427\u001b[39m, in \u001b[36mChatAgent.step\u001b[39m\u001b[34m(self, input_message, response_format)\u001b[39m\n\u001b[32m   1423\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._step_terminate(\n\u001b[32m   1424\u001b[39m         e.args[\u001b[32m1\u001b[39m], tool_call_records, \u001b[33m\"\u001b[39m\u001b[33mmax_tokens_exceeded\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1425\u001b[39m     )\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# Get response from model backend\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_model_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1428\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopenai_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1429\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1430\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurrent_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43miteration_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1431\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtool_schemas\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_full_tool_schemas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprev_num_openai_messages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprev_num_openai_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1434\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1435\u001b[39m prev_num_openai_messages = \u001b[38;5;28mlen\u001b[39m(openai_messages)\n\u001b[32m   1436\u001b[39m iteration_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/camel/agents/chat_agent.py:1816\u001b[39m, in \u001b[36mChatAgent._get_model_response\u001b[39m\u001b[34m(self, openai_messages, num_tokens, current_iteration, response_format, tool_schemas, prev_num_openai_messages)\u001b[39m\n\u001b[32m   1811\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ModelProcessingError(\n\u001b[32m   1812\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to process messages: none of the provided models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1813\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrun successfully.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1814\u001b[39m     )\n\u001b[32m   1815\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response:\n\u001b[32m-> \u001b[39m\u001b[32m1816\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ModelProcessingError(\n\u001b[32m   1817\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to process messages: the only provided model \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1818\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdid not run successfully. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1819\u001b[39m     )\n\u001b[32m   1821\u001b[39m sanitized_messages = \u001b[38;5;28mself\u001b[39m._sanitize_messages_for_logging(\n\u001b[32m   1822\u001b[39m     openai_messages, prev_num_openai_messages\n\u001b[32m   1823\u001b[39m )\n\u001b[32m   1824\u001b[39m logger.info(\n\u001b[32m   1825\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_backend.model_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1826\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mindex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_backend.current_model_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1827\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33miteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_iteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1828\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprocessed these messages: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msanitized_messages\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1829\u001b[39m )\n",
      "\u001b[31mModelProcessingError\u001b[39m: Unable to process messages: the only provided model did not run successfully. Error: Error code: 400 - {'error': {'message': \"Invalid schema for function 'add_memory': In context=('properties', 'metadata', 'type', '0'), 'additionalProperties' is required to be supplied and to be false.\", 'type': 'invalid_request_error', 'param': 'tools[0].function.parameters', 'code': 'invalid_function_parameters'}}"
     ]
    }
   ],
   "source": [
    "# Start interactive chat mode\n",
    "rprint(\"[bold green]üß† Mem0 Cloud Agent is ready![/bold green]\")\n",
    "rprint(\"[dim]Type 'exit' or 'quit' to end the conversation.[/dim]\\n\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_query = input(\"> \")\n",
    "        if user_query.lower() in [\"exit\", \"quit\"]:\n",
    "            rprint(\"[yellow]Goodbye! Your memories are safely stored in Mem0 cloud.[/yellow]\")\n",
    "            break\n",
    "\n",
    "        user_msg = BaseMessage.make_user_message(role_name=\"User\", content=user_query)\n",
    "        response = agent.step(user_msg)\n",
    "        \n",
    "        # Show the agent's response\n",
    "        rprint(f\"[green]ü§ñ Agent:[/green] {response.msg.content}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        rprint(\"\\n[yellow]Goodbye![/yellow]\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ccee4e",
   "metadata": {},
   "source": [
    "## üåü Key Features\n",
    "\n",
    "This memory-enabled agent demonstrates several powerful capabilities:\n",
    "\n",
    "### üß† **Persistent Memory**\n",
    "- Memories are stored in Mem0's cloud infrastructure\n",
    "- Information persists across different chat sessions\n",
    "- Automatic memory organization by agent and user IDs\n",
    "\n",
    "### üîç **Semantic Search**\n",
    "- Find relevant memories using natural language queries\n",
    "- Vector-based semantic matching for better results\n",
    "- Advanced filtering and reranking for improved accuracy\n",
    "\n",
    "### üõ†Ô∏è **Memory Management Tools**\n",
    "- **Add Memory**: Store new information with optional metadata\n",
    "- **Retrieve Memories**: Get all stored memories for the user/agent\n",
    "- **Search Memories**: Find specific information using semantic search\n",
    "- **Delete Memories**: Clear all memories when needed\n",
    "\n",
    "### üéØ **Example Use Cases**\n",
    "- **Personal Assistant**: Remember user preferences, appointments, and important information\n",
    "- **Customer Support**: Maintain context across multiple support sessions\n",
    "- **Learning Companion**: Track learning progress and past topics discussed\n",
    "- **Project Management**: Remember project details, deadlines, and team information\n",
    "\n",
    "## üéÆ Try It Yourself!\n",
    "\n",
    "Here are some example prompts to test with your agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92368555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example interactions you can try:\n",
    "example_prompts = [\n",
    "    \"Remember that I prefer meetings in the morning\",\n",
    "    \"My favorite programming language is Python\",\n",
    "    \"I'm working on a project called 'SmartHome Assistant'\",\n",
    "    \"What do you remember about my work preferences?\",\n",
    "    \"Search for information about programming languages\",\n",
    "    \"Tell me about my current projects\",\n",
    "]\n",
    "\n",
    "for prompt in example_prompts:\n",
    "    print(f\"Try: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce41c6",
   "metadata": {},
   "source": [
    "## üåü Highlights\n",
    "\n",
    "This notebook has guided you through building a sophisticated AI agent with persistent memory capabilities. Key technologies utilized include:\n",
    "\n",
    "- **CAMEL AI**: A powerful multi-agent framework enabling memory-enhanced AI interactions\n",
    "- **Mem0 Cloud**: Cloud-based vector storage with semantic search capabilities\n",
    "- **Custom Toolkits**: Extensible toolkit architecture for adding new capabilities\n",
    "- **Rich UI**: Beautiful terminal output with the Rich library\n",
    "\n",
    "That's everything! Got questions about üê´ CAMEL-AI? [Join us on Discord!](https://discord.gg/CNcNpquyDc) Whether you want to share feedback, explore the latest in multi-agent systems, get support, or connect with others on exciting projects, we'd love to have you in the community! ü§ù \n",
    "\n",
    "Check out some of our other work:\n",
    "- üê´ [Creating Your First CAMEL Agent](https://colab.research.google.com/drive/1AzP33O8rnMW__7ocWJhVBXjKziJXPtim) (free Colab)\n",
    "- [Graph RAG Cookbook](https://colab.research.google.com/drive/1LhI-7a5JUhLXxAfNHzSfvV8uOPl-FLhp) (free Colab)\n",
    "- üßë‚Äç‚öñÔ∏è [Create A Hackathon Judge Committee with Workforce](https://colab.research.google.com/drive/1XpWMxJZ7Kv6RCLaE9RjZH8jdQsyPLzEJ) (free Colab)\n",
    "- üî• [3 Ways to Ingest Data from Websites with Firecrawl & CAMEL](https://colab.research.google.com/drive/1eYTlHkgj9bGbI1d0vYpK0vMSdIlJNRq3) (free Colab)\n",
    "\n",
    "Thanks from everyone at üê´ CAMEL-AI!\n",
    "\n",
    "[CAMEL Homepage](https://www.camel-ai.org/) | [Join Discord](https://discord.gg/CNcNpquyDc) | ‚≠ê [Star us on GitHub](https://github.com/camel-ai/camel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
